<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Huichi Zhou (周辉池)</title>
  
  <meta name="author" content="Huichi Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⭐️</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Huichi Zhou</name>
              </p>
              <p>
                Huichi Zhou is currently a graduate student at <a href="https://www.imperial.ac.uk/">Imperial College London</a> with 
                YangLab advised by Professor <a href="https://scholar.google.com.mx/citations?hl=zh-CN&user=ZfzEFpsAAAAJ&view_op=list_works">Guang Yang</a>.
              </p>
              <p>
                  My research interest is broadly in Adversarial Machine Learning and Large Language Models.
	      </p>
              <p>
                Email: h.zhou24 [at] imperial.ac.uk 
              </p>
	      <p>
                <a href="https://github.com/HuichiZhou">Github</a>
              </p>

	<p>
    <a href="https://scholar.google.com/citations?user=1IJyxpUAAAAJ&hl=en">Google Scholar</a>: 109 <span id="citation-count">109</span>
</p>

<script>
    async function fetchCitationCount() {
        const apiKey = 'bc5a7bbeb0b6bfdd527accad171d9a6620ff54405f3de89a432b6a4735337ff8';  // 替换为你的 SerpApi Key
        const scholarId = '1IJyxpUAAAAJ';   // 替换为你的 Google Scholar ID

        try {
            const response = await fetch(`https://serpapi.com/search.json?engine=google_scholar_author&author_id=${scholarId}&api_key=${apiKey}`);
            const data = await response.json();

            // 获取 citation 数量
            const citationCount = data['cited_by']['table'][0]['citations']['all'];
            document.getElementById('citation-count').innerText = citationCount;
        } catch (error) {
            console.error('Failed to fetch citation count:', error);
            document.getElementById('citation-count').innerText = 'Error';
        }
    }

    fetchCitationCount();
</script>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zhc.jpg"><img style="width:80%;max-width:100%" alt="profile photo" src="images/zhc.jpg" class="hoverZoomLink"></a>
            </td>

          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>&nbsp&nbsp&nbsp&nbsp&nbsp (* denotes equal contribution)
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:30px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="trustrag_stop()" onmouseover="trustrag_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='trustrag_image'>
                  <img src='images/trustrag.jpg' width="220"></div>
                <img src='images/trustrag.jpg' width="220">
              </div>
              <script type="text/javascript">
                function trustrag_start() {
                  document.getElementById('trustrag_image').style.opacity = "1";
                }

                function trustrag_stop() {
                  document.getElementById('trustrag_image').style.opacity = "0";
                }
                trustrag_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> TrustRAG: Enhancing Robustness and Trustworthiness in RAG
              </papertitle>
              </a>
              <br>
              <strong>Huichi Zhou*</strong>,
              Kin-Hei Lee*, 
              Zhonghao Zhan*, 
              Yue Chen,
              Zhenhao Li, 
              Zhaoyang Wang,
              Hamed Haddadi, 
              Emine Yilmaz
              <br>
              <em>Under Review</em>
              <br> 
              <a href="https://trust-rag.github.io/">project page</a>
              /
              <a href="https://arxiv.org/pdf/2501.00879">paper</a>
              /
              <a href="https://github.com/HuichiZhou/TrustRAG">code</a>
              <p></p>
              <p> We introduce TrustRAG, a robust Retrieval-Augmented Generation (RAG) framework. It defends against corpus poisoning attacks by a two-stage mechanism: identifying potential attack patterns with K-means clustering and detecting malicious docs via self-assessment.  </p>
            </td>
          </tr>


          <tr onmouseout="verify_stop()" onmouseover="verify_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='verify_image'>
                  <img src='images/verify.jpg' width="220"></div>
                <img src='images/verify.jpg' width="220">
              </div>
              <script type="text/javascript">
                function verify_start() {
                  document.getElementById('verify_image').style.opacity = "1";
                }

                function verify_stop() {
                  document.getElementById('verify_image').style.opacity = "0";
                }
                verify_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> Verifiable Format Control for Large Language Model Generations
              </papertitle>
              </a>
              <br>
              Zhaoyang Wang*,
              Jinqi jiang*, 
              <strong>Huichi Zhou*</strong>, 
              Wenhao Zheng,
              Xuchao Zhang, 
              Chetan Bansal,
              Huaxiu Yao
              <br>
              <em>NAACL 2025 Findings</em>
              <br> 
              <a href="https://arxiv.org/pdf/2502.04498">paper</a>
              /
              <a href="https://huggingface.co/datasets/jinqij/VFF">code</a>
              <p></p>
              <p> We introduce VFF, which is a dataset and training framework that improves the format-following ability of 7B-level LLMs using Python-based verification and progressive training. It enhances LLMs’ ability to follow specific formats (e.g., JSON) through self-generated data and direct preference optimization (DPO).  </p>
            </td>
          </tr>
	  
	  <tr onmouseout="_stop()" onmouseover="llm4rgnn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='llm4rgnn_image'>
                  <img src='images/llm4rgnn.jpg' width="220"></div>
                <img src='images/llm4rgnn.jpg' width="220">
              </div>
              <script type="text/javascript">
                function llm4rgnn_start() {
                  document.getElementById('llm4rgnn_image').style.opacity = "1";
                }

                function llm4rgnn_stop() {
                  document.getElementById('llm4rgnn_image').style.opacity = "0";
                }
                llm4rgnn_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?
              </papertitle>
              </a>
              <br>
              Zhongjian Zhang*, 
              Xiao Wang*, 
              <strong>Huichi Zhou</strong>, 
              Mengmei Zhang,
              Cheng Yang, 
              Chuan Shi
              <br>
              <em>KDD 2025 Research Track</em>
              <br> 
              <a href="https://arxiv.org/pdf/2408.08685">paper</a>
              /
              <a href="https://github.com/zhongjian-zhang/LLM4RGNN">code</a>
              <p></p>
              <p> LLM4RGNN is a framework that enhances the adversarial robustness of Graph Neural Networks (GNNs) using Large Language Models (LLMs). It distills the inference capability of GPT-4 into a local LLM to identify malicious edges and trains an LM-based edge predictor to find missing important edges. By removing malicious edges and adding important ones, LLM4RGNN significantly improves GNN performance under adversarial attacks, outperforming existing robust GNN frameworks in various datasets and attack scenarios.  </p>
            </td>
          </tr>


	  
	  <tr onmouseout="avllm_stop()" onmouseover="avllm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='avllm_image'>
                  <img src='images/avllm.jpg' width="220"></div>
                <img src='images/avllm.jpg' width="220">
              </div>
              <script type="text/javascript">
                function avllm_start() {
                  document.getElementById('avllm_image').style.opacity = "1";
                }

                function avllm_stop() {
                  document.getElementById('avllm_image').style.opacity = "0";
                }
                avllm_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> Evaluating the Validity of Word-level Adversarial Attacks with Large Language Models
              </papertitle>
              </a>
              <br>
              <strong>Huichi Zhou*</strong>, 
              Zhaoyang Wang*, 
              Hongtao Wang, 
              Dongping Chen,
              Wenhan Mu, 
              Fangyuan Zhang
              <br>
              <em>ACL 2024 Findings</em>
              <br> 
              <a href="https://aclanthology.org/2024.findings-acl.292">paper</a>
              /
              <a href="https://github.com/HuichiZhou/AVLLM">code</a>
              <p></p>
              <p> AVLLM is a framework for evaluating and improving the validity of word-level adversarial attacks on LLMs. It fine-tunes a lightweight LLM to provide a validity score and explanation, helping to generate semantically consistent adversarial examples. AVLLM improves attack quality and consistency through enhanced semantic understanding.  </p>
            </td>
          </tr>

	  	  
	  <tr onmouseout="guiworld_stop()" onmouseover="guiworld_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='guiworld_image'>
                  <img src='images/guiworld.jpg' width="220"></div>
                <img src='images/guiworld.jpg' width="220">
              </div>
              <script type="text/javascript">
                function guiworld_start() {
                  document.getElementById('guiworld_image').style.opacity = "1";
                }

                function guiworld_stop() {
                  document.getElementById('guiworld_image').style.opacity = "0";
                }
                guiworld_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding
              </papertitle>
              </a>
              <br>
              Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, <strong>Huichi Zhou</strong>, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao, Liuyi Chen, Yiqiang Li, Chenlong Wang, Yue Yu, Tianshuo Zhou, Zhen Li, Yi Gui, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun
              <br>
              <em>ICLR 2025</em>
              <br> 
              <a href="https://openreview.net/forum?id=QarKTT5brZ">paper</a>
              /
              <a href="https://github.com/Dongping-Chen/GUI-World">code</a>
              <p></p>
              <p> GUI-WORLD is a comprehensive video-based benchmark and dataset designed for GUI-oriented multimodal understanding. It includes 12,379 GUI videos covering six scenarios (e.g., software, websites, mobile, XR) and eight question types. The dataset captures dynamic and sequential GUI content, addressing challenges faced by existing models. The paper introduces GUI-VID, a fine-tuned Video LLM that improves dynamic GUI understanding but highlights that current models still struggle with complex GUI tasks.  </p>
            </td>
          </tr>



	  <tr onmouseout="mllmjudge_stop()" onmouseover="mllmjudge_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mllmjudge_image'>
                  <img src='images/mllmjudge.jpg' width="220"></div>
                <img src='images/mllmjudge.jpg' width="220">
              </div>
              <script type="text/javascript">
                function mllmjudge_start() {
                  document.getElementById('mllmjudge_image').style.opacity = "1";
                }

                function guiworld_stop() {
                  document.getElementById('mllmjudge_image').style.opacity = "0";
                }
                mllmjudge_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <!-- <a href="https://et-seed.github.io/"> -->
              <papertitle> MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
              </papertitle>
              </a>
              <br>
		Dongping Chen*, Ruoxi Chen*, Shilin Zhang*, Yaochen Wang*, Yinuo Liu*,  <strong>Huichi Zhou*</strong>, Qihui Zhang*, Yao Wan, Pan Zhou, Lichao Sun
              <br>
              <em>ICML 2025 Oral</em>
              <br> 
              <a href="https://openreview.net/pdf?id=dbFEFHAD79">paper</a>
              /
              <a href="https://github.com/Dongping-Chen/MLLM-Judge">code</a>
              <p></p>
              <p> MLLM-as-a-Judge is a benchmark designed to evaluate the judging capabilities of Multimodal Large Language Models (MLLMs) in vision-language tasks. It assesses MLLMs on Scoring Evaluation, Pair Comparison, and Batch Ranking across 14 datasets. Results show that while MLLMs align well with human judgments in pair comparison, they struggle with scoring and batch ranking due to biases and hallucinations. GPT-4V performs best, but the study highlights the need for improvements in consistency and reasoning.  </p>
            </td>
          </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              ICLR 2025
              <br>
              ACL ARR 2024-2025
              <br>
              <!-- Volunteer: <a href="images/wine2020.pdf">WINE <em>2020</em></a> -->
            </td>
          </tr>
        </tbody></table>


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td width="75%" valign="center">
                Teaching Assistant:
            <br>
              &nbsp &nbsp &nbsp <a href="https://deep-generative-models.github.io/">Deep Generative Models</a>, <em> 2020, 2022 </em>
            <br>
                Guest Lecturer:
            <br>
              &nbsp &nbsp &nbsp Frontier Computing Research Practice (Course of Open Source Development), <em> 2024 </em>
            <br>
              &nbsp &nbsp &nbsp Introduction to Computing (Course of Dynamic Programming), <em> 2024 </em>
            </td>
          </tr>

        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Learning 3D Visual Representations for Robotic Manipulation,
              <br>
              &nbsp &nbsp &nbsp <em>National University of Singapore, Feb. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Beijing Normal University, Shanghai Tech University, TeleAI, Jan. 2025</em>
              <br>
              &nbsp &nbsp &nbsp <em>Tsinghua University, Johns Hopkins University, Carnegie Mellon University, Dec. 2024</em>
              <br>
              &nbsp &nbsp &nbsp <em>University of Hong Kong, Nov. 2024</em>
              <br>
              Unified Simulation, Benchmark and Manipulation for Garments, &nbsp&nbsp&nbsp  <em><a href="https://anysyn3d.github.io/about.html">AnySyn3D</a>, 2024</em>
              <br>
                &nbsp &nbsp &nbsp --- If you are interested in 3D Vision research, welcome to follow <a href="https://anysyn3d.github.io/about.html">AnySyn3D</a> that conducts various topics.
              <br>
              Visual Representations for Embodied Agent, &nbsp&nbsp&nbsp  <em>Chinese University of Hong Kong, Shenzhen, 2024</em>
              <br>
              <a href="http://www.csig3dv.net/2024/studentFotum.html">Visual Representations for Embodied Agent</a>, &nbsp&nbsp&nbsp  <em>China3DV, 2024</em>
              <br>
            </td>
          </tr>

        </tbody></table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              Chinese National Scholarship (top1%)  <em> 2023</em>
              <br>
              Excellent Graduation Thesis Award (top1%) <em> 2024 </em>
              <br>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                  <div style="float:right;">
                    Website template comes from <a href="https://jonbarron.info/">Jon Barron</a><br>
                      Last update: March, 2025
                  </div>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
